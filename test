#PROGRAM-9 : 
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
x=np.array([[1,2],[2,3],[3,4],[4,5],[5,6],[6,7],[7,8],[8,9],[9,10],[10,11]])
y=np.array([0,0,0,0,0,1,1,1,1,1])
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)
model=GaussianNB()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
accuracy=accuracy_score(y_test,y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100)) 
#OUTPUT: 
#Accuracy: 100.00%

#PROGRAM-10 :
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
data=load_iris()
X=data.data
y=data.target
binary_indices=np.where(y!=2)
X=X[binary_indices]
y=y[binary_indices]
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)
model=LogisticRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
accuracy=accuracy_score(y_test,y_pred)
print(f"Accuracy : {accuracy:.2f}")
print('Classification report:')
print(classification_report(y_test,y_pred))
print('Confusion Matrix:')
print(confusion_matrix(y_test,y_pred))
X_train_2D=X_train[:,:2]
X_test_2D=X_test[:,:2]
model_2D=LogisticRegression()
model_2D.fit(X_train_2D,y_train)
h=0.2
x_min,x_max=X_train_2D[:,0].min()-1,X_train_2D[:,0].max()+1
y_min,y_max=X_train_2D[:,0].min()-1,X_train_2D[:,0].max()+1
xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))
Z=model_2D.predict(np.c_[xx.ravel(),yy.ravel()])
Z=Z.reshape(xx.shape)
plt.contourf(xx,yy,Z,alpha=0.8)
plt.scatter(X_train_2D[:,0],X_train_2D[:,1],c=y_train,edgecolor='k',marker='o')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision boundary')
plt.show()
#OUTPUT:
Accuracy : 1.00 classification report:
precision	recall f1-score support

0	1.00	1.00	1.00	17
1	1.00	1.00	1.00	13
accuracy	1.00	30
macro avg	1.00	1.00	1.00	30
weighted avg	1.00	1.00	1.00	30
Confusion Matrix 
[[17 0]
[ 0 13]]

#PROGRAM-11 :
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
data = load_iris()
X = data.data
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
base_models = [
    ('decision_tree', DecisionTreeClassifier()),
    ('knn', KNeighborsClassifier()),
    ('svc', SVC(probability=True))
]
meta_model = LogisticRegression()
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)
stacking_model.fit(X_train, y_train)
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
OUTPUT:
Accuracy: 100.00%
Confusion Matrix:
[[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        13
           2       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45

#PROGRAM-12 :
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
from sklearn.svm import SVC
import numpy as np
import pandas as pd
X, Y = make_blobs(n_samples=500, centers=2, random_state=0, cluster_std=0.40)
plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='spring')
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
clf = SVC(kernel='linear')
x = pd.read_csv("path/to/cancer.csv")
if 'malignant' in x.columns and 'benign' in x.columns:
    y = x.iloc[:, 30]. 
    x_features = np.column_stack((x['malignant'], x['benign']))  
    clf.fit(x_features, y)
    prediction1 = clf.predict([[120, 990]])
    prediction2 = clf.predict([[85, 550]])
    print(f"Prediction for [120, 990]: {prediction1}")
    print(f"Prediction for [85, 550]: {prediction2}")
else:
    print("Columns 'malignant' and 'benign' not found in the CSV file")
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='spring')
for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
    yfit = m * xfit + b
    plt.plot(xfit, yfit, '-k')
    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)
plt.xlim(-1, 3.5)
plt.title('Decision Boundaries')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
